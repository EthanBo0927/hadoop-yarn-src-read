# YARN中的资源调度 #

在测试YARN时，使用默认的FIFO调度器，因为是压力测试，一次提交多个作业，运行一段时间后发现作业完成的进度全部都停止了，而作业仍然是活着的。检查后发现，是启动的AppMaster占用了全部内存，RM没有资源分配给Task去运行了。

阅读FIFO调度器后，发现它没有控制最多同时执行作业数的参数。这是让人觉得非常别扭的事情。YARN中现在仅使用内存作为资源限制，以FIFO的调度机制，如果提交的作业多，很容易发生前面描述的状况；因此，在YARN中，FIFO调度器根本就不可用。

> 注：CapacityScheduler支持设置AM占用的最大资源量，默认是0.1。

后来我们使用了CapacityScheduler（能力调度器），顺利完成测试。

但是，以目前仅以内存作为调度依据的作法，在对作业执行的控制上，感觉没有旧版本中单纯的以CPU（Slot可以简单的对应CPU的core）作为调度依据的作法使用方便。不方便的理由是：

1. 作业是用户提交的，内存的申请占用和实际占用都是由用户控制的，用户通常会申请更多内存以保证自己作业的顺利完成，但，这对整个机器的资源有效利用是不利的，可能会导致CPU利用不足（RM是以申请的内存做限制，而不是实际使用的资源，你可以设想自己提交的作业申请一下子占完一台机器全部内存，这样不会再有作业和任务分过来，但你的作业可能实际只消耗了一点点的内存）。

2. 从另一个方面考虑，机器的负载我们一般比较关心，为了让负载在适当范围，我们通常是反过来推算合适的内存申请，而这个推算的申请很可能大于实际的使用值，因为MR程序的Task通常可以把内存消耗控制在一个较小范围内。